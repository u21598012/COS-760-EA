{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fibEHDb5zptq"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets fsspec\n",
        "# Seems to prevent the `Invalid pattern: '**' can only be an entire path component` error\n",
        "# Make sure to manually restart the session\n",
        "\n",
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM8IRRfXsqYG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current working directory:\", current_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqbkNKRrGjD1"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(\"Jupyter's Python Executable:\", sys.executable)\n",
        "print(\"Jupyter's sys.path:\", sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r77DLIkRYIUd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        "    AutoModelForSequenceClassification\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# constants\n",
        "english_pretrained_model_path = './models/zero_shot'\n",
        "emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "MODEL_NAME = \"Davlan/afro-xlmr-base\"\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "class MultiLabelEmotionModel(nn.Module):\n",
        "    \"\"\"Multilabel emotion classification model based on paraphrase-xlm-r-multilingual-v1\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, num_labels: int, dropout_rate: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.config = AutoConfig.from_pretrained(model_name)\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        # Add classification head for multilabel prediction\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
        "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Use [CLS] token representation\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Use BCEWithLogitsLoss for multilabel classification\n",
        "            loss_fn = nn.BCEWithLogitsLoss()\n",
        "            loss = loss_fn(logits, labels.float())\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "class BrighterMultiLabelDataset(Dataset):\n",
        "    \"\"\"Custom dataset for BRIGHTER multilabel emotion data\"\"\"\n",
        "\n",
        "    def __init__(self, texts: List[str], labels: np.ndarray, tokenizer, max_length: int = 512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "def load_and_preprocess_data(lang):\n",
        "    \"\"\"Load and preprocess the BRIGHTER dataset for multilabel classification\"\"\"\n",
        "    print(\"Loading BRIGHTER dataset...\")\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(\"brighter-dataset/BRIGHTER-emotion-categories\", lang)\n",
        "\n",
        "    train_df = pd.DataFrame(dataset['train'])\n",
        "    test_df = pd.DataFrame(dataset['test'])\n",
        "\n",
        "    print(f\"Train samples: {len(train_df)}\")\n",
        "    print(f\"Test samples: {len(test_df)}\")\n",
        "\n",
        "    emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
        "\n",
        "    print(f\"Emotion columns: {emotion_columns}\")\n",
        "\n",
        "    # Check if all emotion columns exist\n",
        "    missing_cols = [col for col in emotion_columns if col not in train_df.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"Warning: Missing columns {missing_cols}\")\n",
        "        print(f\"Available columns: {list(train_df.columns)}\")\n",
        "        emotion_cols = [col for col in train_df.columns if any(emotion in col.lower() for emotion in ['anger', 'disgust', 'fear', 'joy', 'sad', 'surprise', 'happy'])]\n",
        "        print(f\"Potential emotion columns: {emotion_cols}\")\n",
        "        emotion_columns = emotion_cols[:6]  # Take first 6 emotion columns\n",
        "\n",
        "    # Fill None values in emotion columns with 0\n",
        "    # This handles cases where an emotion might not be present and represented as `null`\n",
        "    train_df[emotion_columns] = train_df[emotion_columns].fillna(0)\n",
        "    test_df[emotion_columns] = test_df[emotion_columns].fillna(0)\n",
        "\n",
        "    # Explore emotion distribution\n",
        "    print(\"\\nEmotion distribution in training set:\")\n",
        "    for emotion in emotion_columns:\n",
        "        if emotion in train_df.columns:\n",
        "            count = train_df[emotion].sum()\n",
        "            percentage = (count / len(train_df)) * 100\n",
        "            print(f\"{emotion}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    train_labels = train_df[emotion_columns].values\n",
        "    test_labels = test_df[emotion_columns].values\n",
        "\n",
        "    # Calculate statistics\n",
        "    labels_per_sample = train_labels.sum(axis=1)\n",
        "    print(f\"\\nLabel statistics:\")\n",
        "    print(f\"Average labels per sample: {labels_per_sample.mean():.2f}\")\n",
        "    print(f\"Max labels per sample: {labels_per_sample.max()}\")\n",
        "    print(f\"Samples with no labels: {(labels_per_sample == 0).sum()}\")\n",
        "    print(f\"Samples with multiple labels: {(labels_per_sample > 1).sum()}\")\n",
        "\n",
        "    return train_df, test_df, emotion_columns\n",
        "\n",
        "def compute_multilabel_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for multilabel evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Apply sigmoid to get probabilities\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions)).numpy()\n",
        "\n",
        "    # Convert probabilities to binary predictions (threshold = 0.5)\n",
        "    binary_predictions = (probs > 0.5).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    # Exact match accuracy (all labels must match)\n",
        "    exact_match = accuracy_score(labels, binary_predictions)\n",
        "\n",
        "    # Hamming score - average across all labels\n",
        "    hamming_score = (binary_predictions == labels).mean()\n",
        "\n",
        "    # F1 scores\n",
        "    f1_micro = f1_score(labels, binary_predictions, average='micro')\n",
        "    f1_macro = f1_score(labels, binary_predictions, average='macro')\n",
        "    f1_weighted = f1_score(labels, binary_predictions, average='weighted')\n",
        "\n",
        "    # Per-label accuracy\n",
        "    per_label_acc = []\n",
        "    for i in range(labels.shape[1]):\n",
        "        acc = accuracy_score(labels[:, i], binary_predictions[:, i])\n",
        "        per_label_acc.append(acc)\n",
        "\n",
        "    return {\n",
        "        'exact_match_accuracy': exact_match,\n",
        "        'hamming_score': hamming_score,\n",
        "        'f1_micro': f1_micro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'mean_per_label_accuracy': np.mean(per_label_acc)\n",
        "    }\n",
        "\n",
        "def create_data_loaders(train_df, test_df, emotion_columns, tokenizer, max_length=512):\n",
        "    \"\"\"Create train and test datasets for multilabel classification\"\"\"\n",
        "\n",
        "    train_dataset = BrighterMultiLabelDataset(\n",
        "        texts=train_df['text'].tolist(),\n",
        "        labels=train_df[emotion_columns].values,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    test_dataset = BrighterMultiLabelDataset(\n",
        "        texts=test_df['text'].tolist(),\n",
        "        labels=test_df[emotion_columns].values,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def plot_training_history(trainer):\n",
        "    \"\"\"Plot training metrics\"\"\"\n",
        "    # Extract training history\n",
        "    train_logs = [log for log in trainer.state.log_history if 'train_loss' in log]\n",
        "    eval_logs = [log for log in trainer.state.log_history if 'eval_loss' in log]\n",
        "\n",
        "    train_steps = [log['step'] for log in train_logs]\n",
        "    train_loss = [log['train_loss'] for log in train_logs]\n",
        "\n",
        "    eval_steps = [log['step'] for log in eval_logs]\n",
        "    eval_loss = [log['eval_loss'] for log in eval_logs]\n",
        "    eval_f1_macro = [log['eval_f1_macro'] for log in eval_logs]\n",
        "    hamming_scores = [log['eval_hamming_score'] for log in eval_logs]\n",
        "\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.plot(train_steps, train_loss, label='Training Loss', color='blue')\n",
        "    ax1.plot(eval_steps, eval_loss, label='Validation Loss', color='red')\n",
        "    ax1.set_xlabel('Steps')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot F1 macro\n",
        "    ax2.plot(eval_steps, eval_f1_macro, label='F1 Macro', color='green')\n",
        "    ax2.set_xlabel('Steps')\n",
        "    ax2.set_ylabel('F1 Score')\n",
        "    ax2.set_title('Validation F1 Macro Score')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot Hamming Score\n",
        "    ax3.plot(eval_steps, hamming_scores, label='Hamming Score', color='purple')\n",
        "    ax3.set_xlabel('Steps')\n",
        "    ax3.set_ylabel('Hamming Score')\n",
        "    ax3.set_title('Validation Hamming Score')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "    # Plot both metrics together\n",
        "    ax4.plot(eval_steps, eval_f1_macro, label='F1 Macro', color='green')\n",
        "    ax4.plot(eval_steps, hamming_scores, label='Hamming Score', color='purple')\n",
        "    ax4.set_xlabel('Steps')\n",
        "    ax4.set_ylabel('Score')\n",
        "    ax4.set_title('Combined Validation Metrics')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model_detailed(model, test_dataset, emotion_columns, tokenizer):\n",
        "    \"\"\"Detailed evaluation of the multilabel model\"\"\"\n",
        "    model.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(test_dataset)):\n",
        "            item = test_dataset[i]\n",
        "            input_ids = item['input_ids'].unsqueeze(0).to(device)\n",
        "            attention_mask = item['attention_mask'].unsqueeze(0).to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Apply sigmoid to get probabilities\n",
        "            probabilities = torch.sigmoid(outputs['logits']).cpu().numpy()[0]\n",
        "            binary_predictions = (probabilities > 0.5).astype(int)\n",
        "\n",
        "            all_predictions.append(binary_predictions)\n",
        "            all_probabilities.append(probabilities)\n",
        "            all_labels.append(item['labels'].numpy())\n",
        "\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probabilities = np.array(all_probabilities)\n",
        "\n",
        "    # Calculate comprehensive metrics\n",
        "    exact_match = accuracy_score(all_labels, all_predictions)\n",
        "    hamming_score = (all_predictions == all_labels).mean()\n",
        "    f1_micro = f1_score(all_labels, all_predictions, average='micro',zero_division=0)\n",
        "    f1_macro = f1_score(all_labels, all_predictions, average='macro',zero_division=0)\n",
        "    f1_weighted = f1_score(all_labels, all_predictions, average='weighted',zero_division=0)\n",
        "\n",
        "    print(f\"\\n=== Final Test Results ===\")\n",
        "    print(f\"Exact Match Accuracy: {exact_match:.4f}\")\n",
        "    print(f\"Hamming Score: {hamming_score:.4f}\")\n",
        "    print(f\"F1-Score (Micro): {f1_micro:.4f}\")\n",
        "    print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
        "    print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "    # Per-emotion metrics\n",
        "    print(f\"\\n=== Per-Emotion Results ===\")\n",
        "    for i, emotion in enumerate(emotion_columns):\n",
        "        emotion_f1 = f1_score(all_labels[:, i], all_predictions[:, i], zero_division=0)\n",
        "        emotion_acc = accuracy_score(all_labels[:, i], all_predictions[:, i])\n",
        "        print(f\"{emotion:>8}: F1={emotion_f1:.3f}, Acc={emotion_acc:.3f}\")\n",
        "\n",
        "    # Classification report for each emotion\n",
        "    print(f\"\\n=== Detailed Classification Report ===\")\n",
        "    for i, emotion in enumerate(emotion_columns):\n",
        "        print(f\"\\n{emotion.upper()}:\")\n",
        "\n",
        "        # Explicitly specify labels=[0, 1] to handle cases with only one class present\n",
        "        # Also add zero_division handling to classification_report\n",
        "        print(classification_report(all_labels[:, i], all_predictions[:, i],\n",
        "                                  target_names=['Not Present', 'Present'], digits=3, labels=[0, 1], zero_division=0))\n",
        "\n",
        "    return all_predictions, all_labels, all_probabilities\n",
        "\n",
        "def plot_emotion_distribution(train_df, test_df, emotion_columns):\n",
        "    \"\"\"Plot emotion distribution across train and test sets\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Training set\n",
        "    train_counts = [train_df[col].sum() for col in emotion_columns]\n",
        "    train_percentages = [count/len(train_df)*100 for count in train_counts]\n",
        "\n",
        "    ax1.bar(emotion_columns, train_percentages, color='skyblue', alpha=0.7)\n",
        "    ax1.set_title('Emotion Distribution - Training Set')\n",
        "    ax1.set_ylabel('Percentage of samples')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Test set\n",
        "    test_counts = [test_df[col].sum() for col in emotion_columns]\n",
        "    test_percentages = [count/len(test_df)*100 for count in test_counts]\n",
        "\n",
        "    ax2.bar(emotion_columns, test_percentages, color='lightcoral', alpha=0.7)\n",
        "    ax2.set_title('Emotion Distribution - Test Set')\n",
        "    ax2.set_ylabel('Percentage of samples')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_on_english():\n",
        "    \"\"\"Main training pipeline for multilabel emotion classification\"\"\"\n",
        "    BATCH_SIZE = 4\n",
        "    LEARNING_RATE = 2e-5\n",
        "    NUM_EPOCHS = 20\n",
        "\n",
        "    print(\"=== BRIGHTER Multilabel Emotion Classification Fine-tuning ===\\n\")\n",
        "\n",
        "    # Load and preprocess english data\n",
        "    train_df, test_df, emotion_columns = load_and_preprocess_data(\"eng\")\n",
        "    num_labels = len(emotion_columns)\n",
        "\n",
        "    print(f\"\\nEmotion labels: {emotion_columns}\")\n",
        "    print(f\"Number of labels: {num_labels}\")\n",
        "\n",
        "    # Plot emotion distribution\n",
        "    plot_emotion_distribution(train_df, test_df, emotion_columns)\n",
        "\n",
        "    if os.path.exists(english_pretrained_model_path):\n",
        "        print(f\"\\nLoading fine-tuned model from: {english_pretrained_model_path}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        base_model = MultiLabelEmotionModel(MODEL_NAME, num_labels)\n",
        "        peft_model = PeftModel.from_pretrained(base_model, english_pretrained_model_path)\n",
        "    else:\n",
        "        # Initialize tokenizer\n",
        "        print(f\"\\nLoading tokenizer: {MODEL_NAME}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        # Create datasets\n",
        "        print(\"Creating datasets...\")\n",
        "        train_dataset, test_dataset = create_data_loaders(train_df, test_df, emotion_columns, tokenizer, MAX_LENGTH)\n",
        "\n",
        "        # Initialize model\n",
        "        print(f\"Initializing multilabel model with {num_labels} emotion labels...\")\n",
        "        model = MultiLabelEmotionModel(MODEL_NAME, num_labels)\n",
        "\n",
        "        # LoRA configurations\n",
        "        lora_config = LoraConfig(\n",
        "            r=16,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"query\", \"value\", \"key\"], # Changes depending on the model\n",
        "            lora_dropout=0.1,\n",
        "            bias=\"none\",\n",
        "            task_type=\"SEQ_CLS\"\n",
        "        )\n",
        "\n",
        "        peft_model = get_peft_model(model, lora_config)\n",
        "        peft_model.print_trainable_parameters()\n",
        "\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir='./results',\n",
        "            num_train_epochs=NUM_EPOCHS,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            warmup_steps=100,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir='./logs',\n",
        "            logging_steps=50,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=500,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=500,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_f1_macro\",\n",
        "            greater_is_better=True,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            dataloader_num_workers=2,\n",
        "            dataloader_pin_memory=True,\n",
        "            remove_unused_columns=False,\n",
        "            report_to=\"none\",\n",
        "        )\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = Trainer(\n",
        "            model=peft_model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            compute_metrics=compute_multilabel_metrics,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"CUDA available:\", torch.cuda.is_available())\n",
        "        print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "        print(\"\\nStarting training...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Plot training history\n",
        "        print(\"\\nPlotting training history...\")\n",
        "        plot_training_history(trainer)\n",
        "\n",
        "        # Detailed evaluation\n",
        "        print(\"\\nPerforming detailed evaluation...\")\n",
        "        predictions, labels, probabilities = evaluate_model_detailed(peft_model, test_dataset, emotion_columns, tokenizer)\n",
        "\n",
        "        # Save the model\n",
        "        print(\"\\nSaving model...\")\n",
        "        peft_model.save_pretrained('./models/zero_shot')\n",
        "        tokenizer.save_pretrained('./models/zero_shot')\n",
        "\n",
        "        # Save emotion columns\n",
        "        import json\n",
        "        with open('./models/zero_shot/emotion_columns.json', 'w') as f:\n",
        "            json.dump(emotion_columns, f)\n",
        "\n",
        "        print(\"\\nTraining completed! Model saved to './models/zero_shot'\")\n",
        "\n",
        "    #------------------\n",
        "\n",
        "    return peft_model, tokenizer, emotion_columns\n",
        "\n",
        "def predict_emotions(text: str, model, tokenizer, emotion_columns, threshold=0.5, device='cpu'):\n",
        "    \"\"\"Predict emotions for a single text with probabilities\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probabilities = torch.sigmoid(outputs['logits']).cpu().numpy()[0]\n",
        "\n",
        "    # Get predictions above threshold\n",
        "    predictions = probabilities > threshold\n",
        "\n",
        "    results = []\n",
        "    for i, (emotion, prob, pred) in enumerate(zip(emotion_columns, probabilities, predictions)):\n",
        "        results.append({\n",
        "            'emotion': emotion,\n",
        "            'probability': prob,\n",
        "            'predicted': bool(pred)\n",
        "        })\n",
        "\n",
        "    # Sort by probability (descending)\n",
        "    results.sort(key=lambda x: x['probability'], reverse=True)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0Jw-alWJU8q"
      },
      "outputs": [],
      "source": [
        "def finetune_on_hausa(pretrained_model, tokenizer, emotion_columns):\n",
        "    \"\"\"Phase 2: Fine-tune the English-trained model on Hausa data.\"\"\"\n",
        "    BATCH_SIZE = 4\n",
        "    LEARNING_RATE = 1e-5\n",
        "    NUM_EPOCHS = 20\n",
        "    hausa_finetuned_model_path = './models/fine_tuned_hausa'\n",
        "    output_path = './models/fine_tuned_hausa'\n",
        "\n",
        "    print(\"\\n=== PHASE 2: Fine-tuning on Hausa Data ===\")\n",
        "\n",
        "     # Load Hausa data\n",
        "    train_df, test_df, _ = load_and_preprocess_data(\"hau\")\n",
        "    train_dataset, test_dataset = create_data_loaders(train_df, test_df, emotion_columns, tokenizer, MAX_LENGTH)\n",
        "\n",
        "    if os.path.exists(hausa_finetuned_model_path):\n",
        "        num_labels = len(emotion_columns)\n",
        "        print(f\"\\nLoading fine-tuned model from: {hausa_finetuned_model_path}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        base_model = MultiLabelEmotionModel(MODEL_NAME, num_labels)\n",
        "        pretrained_model = PeftModel.from_pretrained(base_model, hausa_finetuned_model_path)\n",
        "    else:\n",
        "        # LoRA config (assumes pretrained_model is already a PEFT model)\n",
        "      training_args = TrainingArguments(\n",
        "          output_dir='./results_hausa',\n",
        "          num_train_epochs=NUM_EPOCHS,\n",
        "          per_device_train_batch_size=BATCH_SIZE,\n",
        "          per_device_eval_batch_size=BATCH_SIZE,\n",
        "          warmup_steps=100,\n",
        "          weight_decay=0.01,\n",
        "          logging_dir='./logs_hausa',\n",
        "          logging_steps=50,\n",
        "          eval_strategy=\"steps\",\n",
        "          eval_steps=1000,\n",
        "          save_strategy=\"steps\",\n",
        "          save_steps=1000,\n",
        "          load_best_model_at_end=True,\n",
        "          metric_for_best_model=\"eval_f1_macro\",\n",
        "          greater_is_better=True,\n",
        "          learning_rate=LEARNING_RATE,\n",
        "          fp16=torch.cuda.is_available(),\n",
        "          dataloader_num_workers=2,\n",
        "          dataloader_pin_memory=True,\n",
        "          remove_unused_columns=False,\n",
        "          report_to=\"none\",\n",
        "      )\n",
        "\n",
        "      trainer = Trainer(\n",
        "          model=pretrained_model,\n",
        "          args=training_args,\n",
        "          train_dataset=train_dataset,\n",
        "          eval_dataset=test_dataset,\n",
        "          compute_metrics=compute_multilabel_metrics,\n",
        "          callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "      )\n",
        "\n",
        "      print(\"\\nStarting Hausa fine-tuning...\")\n",
        "      trainer.train()\n",
        "\n",
        "      # Save Hausa fine-tuned model\n",
        "      pretrained_model.save_pretrained(output_path)\n",
        "      tokenizer.save_pretrained(output_path)\n",
        "\n",
        "      print(\"\\nHausa fine-tuned model saved to:\", output_path)\n",
        "\n",
        "      # Detailed evaluation\n",
        "      print(\"\\nPerforming detailed evaluation...\")\n",
        "      predictions, labels, probabilities = evaluate_model_detailed(pretrained_model, test_dataset, emotion_columns, tokenizer)\n",
        "\n",
        "    return pretrained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1Iltao8K8mH"
      },
      "outputs": [],
      "source": [
        "class LimeMultiLabelEmotionExplainer:\n",
        "    \"\"\"\n",
        "    LIME explainer wrapper for multilabel emotion classification models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, emotion_columns: List[str], device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        \"\"\"\n",
        "        Initialize the LIME explainer\n",
        "\n",
        "        Args:\n",
        "            model: Your trained MultiLabelEmotionModel (or PeftModel)\n",
        "            tokenizer: The tokenizer used for preprocessing\n",
        "            emotion_columns: List of emotion label names\n",
        "            device: Device to run inference on\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.emotion_columns = emotion_columns\n",
        "        self.device = device\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Initialize LIME explainer\n",
        "        self.explainer = LimeTextExplainer(\n",
        "            class_names=emotion_columns\n",
        "        )\n",
        "\n",
        "    def predict_proba(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Prediction function for LIME - returns probabilities for all emotions\n",
        "\n",
        "        Args:\n",
        "            texts: List of text strings to predict\n",
        "\n",
        "        Returns:\n",
        "            numpy array of shape (n_samples, n_emotions) with probabilities\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for text in texts:\n",
        "                # Tokenize the text\n",
        "                inputs = self.tokenizer(\n",
        "                    text,\n",
        "                    return_tensors='pt',\n",
        "                    truncation=True,\n",
        "                    padding=True,\n",
        "                    max_length=512\n",
        "                ).to(self.device)\n",
        "\n",
        "                # Get model predictions\n",
        "                outputs = self.model(**inputs)\n",
        "                logits = outputs['logits']\n",
        "\n",
        "                # Convert to probabilities using sigmoid (for multilabel)\n",
        "                probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "                predictions.append(probs)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def explain_instance(self, text: str, top_labels: int = None, num_features: int = 10,\n",
        "                        num_samples: int = 1000) -> Dict:\n",
        "        \"\"\"\n",
        "        Explain a single text instance\n",
        "\n",
        "        Args:\n",
        "            text: Text to explain\n",
        "            top_labels: Number of top emotions to explain (None for all)\n",
        "            num_features: Number of words to include in explanation\n",
        "            num_samples: Number of samples for LIME\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing explanations and predictions\n",
        "        \"\"\"\n",
        "        # Get prediction for the original text\n",
        "        original_pred = self.predict_proba([text])[0]\n",
        "\n",
        "        # Determine which emotions to explain\n",
        "        if top_labels is None:\n",
        "            labels_to_explain = list(range(len(self.emotion_columns)))\n",
        "        else:\n",
        "            # Get top predicted emotions\n",
        "            top_indices = np.argsort(original_pred)[-top_labels:][::-1]\n",
        "            labels_to_explain = top_indices.tolist()\n",
        "\n",
        "        # Generate LIME explanation\n",
        "        explanation = self.explainer.explain_instance(\n",
        "            text,\n",
        "            self.predict_proba,\n",
        "            labels=labels_to_explain,\n",
        "            num_features=num_features,\n",
        "            num_samples=num_samples\n",
        "        )\n",
        "\n",
        "        # Format results\n",
        "        results = {\n",
        "            'text': text,\n",
        "            'predictions': {emotion: float(prob) for emotion, prob in zip(self.emotion_columns, original_pred)},\n",
        "            'explanations': {}\n",
        "        }\n",
        "\n",
        "        # Extract explanations for each emotion\n",
        "        for label_idx in labels_to_explain:\n",
        "            emotion_name = self.emotion_columns[label_idx]\n",
        "            word_importance = explanation.as_list(label=label_idx)\n",
        "            results['explanations'][emotion_name] = word_importance\n",
        "\n",
        "        return results\n",
        "\n",
        "    def visualize_explanation(self, explanation_result: Dict, emotion: str = None,\n",
        "                            save_path: str = None, figsize: Tuple[int, int] = (12, 8)):\n",
        "        \"\"\"\n",
        "        Visualize LIME explanations\n",
        "\n",
        "        Args:\n",
        "            explanation_result: Result from explain_instance\n",
        "            emotion: Specific emotion to visualize (None for all)\n",
        "            save_path: Path to save the plot\n",
        "            figsize: Figure size\n",
        "        \"\"\"\n",
        "        explanations = explanation_result['explanations']\n",
        "        predictions = explanation_result['predictions']\n",
        "\n",
        "        if emotion and emotion in explanations:\n",
        "            emotions_to_plot = [emotion]\n",
        "        else:\n",
        "            emotions_to_plot = list(explanations.keys())\n",
        "\n",
        "        n_emotions = len(emotions_to_plot)\n",
        "        fig, axes = plt.subplots(n_emotions, 1, figsize=figsize, squeeze=False)\n",
        "\n",
        "        for i, emotion_name in enumerate(emotions_to_plot):\n",
        "            ax = axes[i, 0]\n",
        "            word_importance = explanations[emotion_name]\n",
        "\n",
        "            # Separate positive and negative influences\n",
        "            words = [item[0] for item in word_importance]\n",
        "            scores = [item[1] for item in word_importance]\n",
        "\n",
        "            # Create color map based on positive/negative influence\n",
        "            colors = ['green' if score > 0 else 'red' for score in scores]\n",
        "\n",
        "            # Create horizontal bar plot\n",
        "            y_pos = np.arange(len(words))\n",
        "            bars = ax.barh(y_pos, scores, color=colors, alpha=0.7)\n",
        "\n",
        "            ax.set_yticks(y_pos)\n",
        "            ax.set_yticklabels(words)\n",
        "            ax.set_xlabel('Importance Score')\n",
        "            ax.set_title(f'{emotion_name} (Prediction: {predictions[emotion_name]:.3f})')\n",
        "            ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "            # Add vertical line at x=0\n",
        "            ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.suptitle(f'LIME Explanations\\nText: \"{explanation_result[\"text\"][:100]}...\"',\n",
        "                     y=1.02, fontsize=12)\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "def demo_lime_usage(model, tokenizer, emotion_columns):\n",
        "    explainer = LimeMultiLabelEmotionExplainer(model, tokenizer, emotion_columns)\n",
        "    text = \"I'm so happy and excited!\"\n",
        "    explanation = explainer.explain_instance(text, top_labels=6)\n",
        "    explainer.visualize_explanation(explanation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CHw4BlSx7ju"
      },
      "outputs": [],
      "source": [
        "# Run the main training pipeline\n",
        "english_model, tokenizer, emotion_columns = train_on_english()\n",
        "hausa_model = finetune_on_hausa(english_model, tokenizer, emotion_columns)\n",
        "\n",
        "\n",
        "print(\"\\n=== Example Multilabel Predictions ===\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# load and preprocess hausa data\n",
        "train_df, test_df, emotion_columns = load_and_preprocess_data(\"hau\")\n",
        "print(test_df.head(5))\n",
        "\n",
        "demo_lime_usage(hausa_model, tokenizer, emotion_columns)\n",
        "\n",
        "# Test zero shot inference\n",
        "train_dataset, test_dataset = create_data_loaders(train_df, test_df, emotion_columns, tokenizer, MAX_LENGTH)\n",
        "predictions, labels, probabilities = evaluate_model_detailed(english_model, test_dataset, emotion_columns, tokenizer)\n",
        "\n",
        "# Example predictions\n",
        "test_texts = [\n",
        "    \"I am so happy and excited about this wonderful news!\",\n",
        "    \"This makes me really angry and disgusted.\",\n",
        "    \"This is absolutely baffling.\",\n",
        "    \"What a joyful and surprising moment!\",\n",
        "    \"I'm feeling quite sad and afraid lately.\",\n",
        "    \"I can't believe this happened, I'm shocked and happy at the same time!\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    print(f\"\\nText: '{text}'\")\n",
        "    results = predict_emotions(text, english_model, tokenizer, emotion_columns, threshold=0.3, device=device)\n",
        "\n",
        "    print(\"Predicted emotions:\")\n",
        "    for result in results:\n",
        "        status = \"âœ“\" if result['predicted'] else \" \"\n",
        "        print(f\"  {status} {result['emotion']:>8}: {result['probability']:.3f}\")\n",
        "\n",
        "    predicted_emotions = [r['emotion'] for r in results if r['predicted']]\n",
        "    print(f\"Final prediction: {predicted_emotions if predicted_emotions else 'No emotions detected'}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chCUi0JeEAI3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from torch.nn.functional import sigmoid\n",
        "\n",
        "def predict_proba(texts):\n",
        "    \"\"\"Wraps the model for LIME\"\"\"\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = hausa_model(**tokens)\n",
        "        logits = outputs['logits'] if isinstance(outputs, dict) else outputs.logits\n",
        "        probs = sigmoid(logits).cpu().numpy()  # For multilabel use sigmoid\n",
        "    return probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56MW2s2uEiwe"
      },
      "outputs": [],
      "source": [
        "# Define class names\n",
        "class_names = emotion_columns\n",
        "\n",
        "# Create the explainer\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "# Choose a sample text\n",
        "sample_text = \"hmmm wai hauka, kamar qasa mai mutum sama da 200m ake maganar wai wannan shine jaririn farko? wllh ba gaskiya bane shidai suka fada kawaiðŸ¤”\"\n",
        "\n",
        "# Explain prediction\n",
        "exp = explainer.explain_instance(sample_text, predict_proba, num_features=10, labels=list(range(len(class_names))))\n",
        "\n",
        "# Show explanation for each label\n",
        "for label_index in range(len(class_names)):\n",
        "    print(f\"\\nExplanation for label: {class_names[label_index]}\")\n",
        "    exp.show_in_notebook(text=sample_text, labels=[label_index])  # Or use exp.as_list(label=label_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "lDciD3AeQR-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"KhweziSandi/XLM-R-Zero-Shot-Hausa\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "base_model = MultiLabelEmotionModel(MODEL_NAME, len(emotion_columns))\n",
        "model_to_eval = PeftModel.from_pretrained(base_model, path)\n",
        "\n",
        "train_df, test_df, _ = load_and_preprocess_data(\"hau\")\n",
        "train_dataset, test_dataset = create_data_loaders(train_df, test_df, emotion_columns, tokenizer, MAX_LENGTH)\n",
        "predictions, labels, probabilities = evaluate_model_detailed(model_to_eval, test_dataset, emotion_columns, tokenizer)"
      ],
      "metadata": {
        "id": "_15H2a82QVVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9wEVeKs4wtrJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}